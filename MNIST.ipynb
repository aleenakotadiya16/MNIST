{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8c1350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68bb63a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9af8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8861134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aleena\\anaconda3\\envs\\tf1_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Aleena\\anaconda3\\envs\\tf1_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Aleena\\anaconda3\\envs\\tf1_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\Aleena\\anaconda3\\envs\\tf1_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Aleena\\anaconda3\\envs\\tf1_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Aleena\\anaconda3\\envs\\tf1_env\\lib\\site-packages\\tensorflow_core\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bafae4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab4352b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c2f72dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67cb7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize_weights(filter_shape):\n",
    "    init_random_dist=tf.truncated_normal(filter_shape,stddev=0.1)\n",
    "    return(tf.Variable(init_random_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcfd0c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(bias_shape):\n",
    "    initail_bias_vals=tf.constant(0.1,shape=bias_shape)\n",
    "    return(tf.Variable(initail_bias_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37406eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convolution_layer_and_compute_dot_products(inputs,filter_shape):\n",
    "    filter_intialized_with_weights=initalize_weights(filter_shape)\n",
    "    conv_layer_outputs=tf.nn.conv2d(inputs,filter_intialized_with_weights,strides=[1,1,1,1],padding='SAME')\n",
    "    return (conv_layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6091ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relu_layer_and_compute_dotproduct_plus_b(inputs,filter_shape):\n",
    "    b=initialize_bias([filter_shape[3]])\n",
    "    relu_layer_outputs=tf.nn.relu(inputs + b)\n",
    "    return(relu_layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d779f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maxpool2by2_and_reduce_spatial_size(inputs):\n",
    "    pooling_layer_outputs=tf.nn.max_pool(inputs,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    return (pooling_layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e07db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_connected_layer_and_compute_dotproduct_plus_bias(inputs,output_size):\n",
    "    input_size=int(inputs.get_shape()[1])\n",
    "    W=initalize_weights([input_size,output_size])\n",
    "    b=initialize_bias([output_size])\n",
    "    fc_xW_plus_bias_outputs=tf.matmul(inputs,W)+b\n",
    "    return (fc_xW_plus_bias_outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f05ac077",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,shape=[None,784])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c69309b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true=tf.placeholder(tf.float32,[None,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8019799",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image=tf.reshape(x,[-1,28,28,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd4e87",
   "metadata": {},
   "source": [
    "Create 1st Convolutional Layer,ReLu Layer and perform computation: x*W+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90a67e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_1_outputs = create_convolution_layer_and_compute_dot_products(x_image,filter_shape=[5,5,1,32])\n",
    "conv_relu_layer_1_outputs \\\n",
    "    = create_relu_layer_and_compute_dotproduct_plus_b(conv_layer_1_outputs,filter_shape=[5,5,1,32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30ea47",
   "metadata": {},
   "source": [
    "## Create 1st Pooling Layer and Reduce Spatial Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91fbb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer_1_outputs = create_maxpool2by2_and_reduce_spatial_size(conv_relu_layer_1_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ec978",
   "metadata": {},
   "source": [
    "## Create 1st Convolutional Layer,ReLu Layer and perform computation: x*W+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54669e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_2_outputs = create_convolution_layer_and_compute_dot_products(pooling_layer_1_outputs,filter_shape=[5,5,32,64])\n",
    "                \n",
    "conv_relu_layer_2_outputs \\\n",
    "    = create_relu_layer_and_compute_dotproduct_plus_b(conv_layer_2_outputs,filter_shape=[5,5,32,64])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c907",
   "metadata": {},
   "source": [
    "### Create 1st Pooling Layer and Reduce Spatial Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4924e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer_2_outputs = create_maxpool2by2_and_reduce_spatial_size(conv_relu_layer_2_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9034841d",
   "metadata": {},
   "source": [
    "### Reshape/flatten data making it ready to be fed into 1st FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ec5752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_layer_2_outputs_flat=tf.reshape(pooling_layer_2_outputs,[-1,7*7*64])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fac9d9",
   "metadata": {},
   "source": [
    "### create Droupout layer and dropout a fraction of outputs randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9069e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_1_outputs \\\n",
    "    =create_fully_connected_layer_and_compute_dotproduct_plus_bias(pooling_layer_2_outputs_flat,output_size=1024)\n",
    "fc_relu_layer_1_outputs=tf.nn.relu(fc_layer_1_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e2a14",
   "metadata": {},
   "source": [
    "### create dropout layer and dropout a fraction of outputs randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e573f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-46-ab52f98e866d>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)\n",
    "fc_dropout_outputs=tf.nn.dropout(fc_layer_1_outputs,keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd9cf7",
   "metadata": {},
   "source": [
    "### Create Final FC layer compute(x.W +b), and produce Final Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f45571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=create_fully_connected_layer_and_compute_dotproduct_plus_bias(fc_dropout_outputs,output_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba912821",
   "metadata": {},
   "source": [
    "### Define Loss Function and Calculate Softmax Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2b564e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-48-1c89e136e6f4>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax_cross_entropy_loss=tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)\n",
    "cross_entropy_mean=tf.reduce_mean(softmax_cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16302f9",
   "metadata": {},
   "source": [
    "### Create an Optimizer to Optimize CNN model and set learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1dd5edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf30c9",
   "metadata": {},
   "source": [
    "### Create a Trainer to Ttaining CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "731ad0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_trainer=optimizer.minimize(cross_entropy_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd315d3",
   "metadata": {},
   "source": [
    "### Create a Variable initializer to initaialize ALL variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78f33dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_initializer=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766c4ac",
   "metadata": {},
   "source": [
    "### Set the Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8eb2c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d449e1",
   "metadata": {},
   "source": [
    "### Run tf.session() to Train and Test Deep learning CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "047897d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On STEP : 0\n",
      "Accuracy :\n",
      "0.1204\n",
      "\n",
      "\n",
      "On STEP : 100\n",
      "Accuracy :\n",
      "0.9323\n",
      "\n",
      "\n",
      "On STEP : 200\n",
      "Accuracy :\n",
      "0.9576\n",
      "\n",
      "\n",
      "On STEP : 300\n",
      "Accuracy :\n",
      "0.966\n",
      "\n",
      "\n",
      "On STEP : 400\n",
      "Accuracy :\n",
      "0.9682\n",
      "\n",
      "\n",
      "On STEP : 500\n",
      "Accuracy :\n",
      "0.9728\n",
      "\n",
      "\n",
      "On STEP : 600\n",
      "Accuracy :\n",
      "0.9746\n",
      "\n",
      "\n",
      "On STEP : 700\n",
      "Accuracy :\n",
      "0.9787\n",
      "\n",
      "\n",
      "On STEP : 800\n",
      "Accuracy :\n",
      "0.9756\n",
      "\n",
      "\n",
      "On STEP : 900\n",
      "Accuracy :\n",
      "0.974\n",
      "\n",
      "\n",
      "On STEP : 1000\n",
      "Accuracy :\n",
      "0.9792\n",
      "\n",
      "\n",
      "On STEP : 1100\n",
      "Accuracy :\n",
      "0.9767\n",
      "\n",
      "\n",
      "On STEP : 1200\n",
      "Accuracy :\n",
      "0.9795\n",
      "\n",
      "\n",
      "On STEP : 1300\n",
      "Accuracy :\n",
      "0.9761\n",
      "\n",
      "\n",
      "On STEP : 1400\n",
      "Accuracy :\n",
      "0.9805\n",
      "\n",
      "\n",
      "On STEP : 1500\n",
      "Accuracy :\n",
      "0.9823\n",
      "\n",
      "\n",
      "On STEP : 1600\n",
      "Accuracy :\n",
      "0.9753\n",
      "\n",
      "\n",
      "On STEP : 1700\n",
      "Accuracy :\n",
      "0.9829\n",
      "\n",
      "\n",
      "On STEP : 1800\n",
      "Accuracy :\n",
      "0.981\n",
      "\n",
      "\n",
      "On STEP : 1900\n",
      "Accuracy :\n",
      "0.9811\n",
      "\n",
      "\n",
      "On STEP : 2000\n",
      "Accuracy :\n",
      "0.9841\n",
      "\n",
      "\n",
      "On STEP : 2100\n",
      "Accuracy :\n",
      "0.9818\n",
      "\n",
      "\n",
      "On STEP : 2200\n",
      "Accuracy :\n",
      "0.9827\n",
      "\n",
      "\n",
      "On STEP : 2300\n",
      "Accuracy :\n",
      "0.9837\n",
      "\n",
      "\n",
      "On STEP : 2400\n",
      "Accuracy :\n",
      "0.9864\n",
      "\n",
      "\n",
      "On STEP : 2500\n",
      "Accuracy :\n",
      "0.9837\n",
      "\n",
      "\n",
      "On STEP : 2600\n",
      "Accuracy :\n",
      "0.9846\n",
      "\n",
      "\n",
      "On STEP : 2700\n",
      "Accuracy :\n",
      "0.9854\n",
      "\n",
      "\n",
      "On STEP : 2800\n",
      "Accuracy :\n",
      "0.9866\n",
      "\n",
      "\n",
      "On STEP : 2900\n",
      "Accuracy :\n",
      "0.9852\n",
      "\n",
      "\n",
      "On STEP : 3000\n",
      "Accuracy :\n",
      "0.9848\n",
      "\n",
      "\n",
      "On STEP : 3100\n",
      "Accuracy :\n",
      "0.9859\n",
      "\n",
      "\n",
      "On STEP : 3200\n",
      "Accuracy :\n",
      "0.986\n",
      "\n",
      "\n",
      "On STEP : 3300\n",
      "Accuracy :\n",
      "0.9857\n",
      "\n",
      "\n",
      "On STEP : 3400\n",
      "Accuracy :\n",
      "0.9856\n",
      "\n",
      "\n",
      "On STEP : 3500\n",
      "Accuracy :\n",
      "0.986\n",
      "\n",
      "\n",
      "On STEP : 3600\n",
      "Accuracy :\n",
      "0.9864\n",
      "\n",
      "\n",
      "On STEP : 3700\n",
      "Accuracy :\n",
      "0.9858\n",
      "\n",
      "\n",
      "On STEP : 3800\n",
      "Accuracy :\n",
      "0.985\n",
      "\n",
      "\n",
      "On STEP : 3900\n",
      "Accuracy :\n",
      "0.9873\n",
      "\n",
      "\n",
      "On STEP : 4000\n",
      "Accuracy :\n",
      "0.9837\n",
      "\n",
      "\n",
      "On STEP : 4100\n",
      "Accuracy :\n",
      "0.9893\n",
      "\n",
      "\n",
      "On STEP : 4200\n",
      "Accuracy :\n",
      "0.9876\n",
      "\n",
      "\n",
      "On STEP : 4300\n",
      "Accuracy :\n",
      "0.9876\n",
      "\n",
      "\n",
      "On STEP : 4400\n",
      "Accuracy :\n",
      "0.9849\n",
      "\n",
      "\n",
      "On STEP : 4500\n",
      "Accuracy :\n",
      "0.9849\n",
      "\n",
      "\n",
      "On STEP : 4600\n",
      "Accuracy :\n",
      "0.9882\n",
      "\n",
      "\n",
      "On STEP : 4700\n",
      "Accuracy :\n",
      "0.989\n",
      "\n",
      "\n",
      "On STEP : 4800\n",
      "Accuracy :\n",
      "0.9874\n",
      "\n",
      "\n",
      "On STEP : 4900\n",
      "Accuracy :\n",
      "0.988\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(vars_initializer)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        batch_x,batch_y=mnist.train.next_batch(50)\n",
    "        sess.run(cnn_trainer,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.5})\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('On STEP : {}'.format(i))\n",
    "            print('Accuracy :')\n",
    "            \n",
    "            matches =tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n",
    "            acc =tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "            test_accuracy = sess.run( acc, feed_dict={x:mnist.test.images,\\\n",
    "                                                     y_true:mnist.test.labels,\\\n",
    "                                                     hold_prob:1.0})\n",
    "            print(test_accuracy)\n",
    "            print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68caaf64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf1_env)",
   "language": "python",
   "name": "tf1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
